{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlaOye23/Mauctree/blob/main/Auctree_data_acquisition_and_storage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NI6nG-zRjQOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Acquire and store Auctree.com data\n",
        "1)Webscrape Auctree.com,\n",
        "\n",
        "2)poplulate a flatfile, \n",
        "\n",
        "3)upload to cloud storage, \n",
        "\n",
        "4)populate a database."
      ],
      "metadata": {
        "id": "WHxYjGFOjR7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80b4fmAW84G5"
      },
      "outputs": [],
      "source": [
        "#install relevant packages\n",
        "!pip install firebase_admin\n",
        "!pip install azure-storage-blob\n",
        "AZR_CONN_STR = os.getenv('AZURE_STORAGE_CONNECTION_STRING')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce-By7Vx9E6P"
      },
      "source": [
        "#**Attempt 1: Scraping the list view **\n",
        "\n",
        "**problems**:- no images, limited fields of info, only 5 propeties appeared\n",
        "\n",
        "status:- largely resolved in attempt 2\n",
        "\n",
        "#**SKIP TO ATTEMPT 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "WQCwWSAwbz2k",
        "outputId": "6ba6840f-617d-4d41-c312-e5419acf28f4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#Python program to scrape website \\n#and save quotes from website \\nimport requests \\nfrom bs4 import BeautifulSoup\\nfrom IPython.core.display import HTML\\n\\nstyles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\\nHTML(styles)\\n\\n#GET HTML DOM contents\\nURL = \"https://auctree.com/en/\"#URL for getting BDC rates from abokiFX.com\\nr = requests.get(URL) #response to the HTTP request on the abokiFX site\\nsoup = BeautifulSoup(r.content, \\'html5lib\\') #\\'soupify\\' for traversal using bs4\\n\\n\\nbody = soup.find(\\'div\\', attrs = {\\'class\\': \\'css-t23ds5\\'}) \\n\\nprint(body)\\n\\nnames = []\\nnames_ = body.findAll(\\'span\\', attrs = {\\'id\\': \\'SearchTableName\\'}) \\nfor name in names_:\\n  names.append(name.text)\\n\\nprop_types = []\\nprop_types_ = body.findAll(\\'span\\', attrs = {\\'id\\': \\'SearchTablePropertyTypes\\'}) \\nfor prop_type in prop_types_:\\n  prop_types.append(prop_type.text)\\n\\nregions = []\\nregions_ = body.findAll(\\'span\\', attrs = {\\'id\\': \\'SearchTableRegion\\'})\\nfor region in regions_:\\n  regions.append(region.text)\\n\\ncities = []\\ncities_ = body.findAll(\\'span\\', attrs = {\\'id\\': \\'SearchTableCities\\'})\\nfor city in cities_:\\n  cities.append(city.text)\\n\\nsale_types = []\\nsale_types_ = body.findAll(\\'span\\', attrs = {\\'id\\': \\'SearchTableSaleType\\'}) \\nfor sale_type in sale_types_:\\n  sale_types.append(sale_type.text)\\n\\nauc_prices = []\\nauc_prices_ = body.findAll(\\'span\\', attrs = {\\'id\\': \\'SearchTableAuctionPrice\\'}) \\nfor auc_price in auc_prices_:\\n  auc_prices.append(auc_price.text)\\n\\ntime_lefts = []\\ntime_lefts_ = body.findAll(\\'span\\', attrs = {\\'id\\': \\'SearchTableUntil\\'}) \\nfor time_left in time_lefts_:\\n  time_lefts.append(time_left.text)\\n\\nend_dates = []\\nend_dates_ = body.findAll(\\'span\\', attrs = {\\'id\\': \\'SearchTableEndDate\\'}) \\nfor end_date in end_dates_:\\n  end_dates.append(end_date.text)\\n\\nnumbers = []\\nfor name in names:\\n  number = name[:name.find(\\'.\\')]\\n  numbers.append(number)\\n\\nnumbers = []\\nfor i in range(len(names)):\\n  number = (names[i])[:names[i].find(\\'.\\')]\\n  names[i] = (names[i])[names[i].find(\\'Cl\\')+3:]# ignore number + cl prefix\\n  numbers.append(number)\\n\\nimages = []\\nfor number in numbers:\\n  #GET HTML DOM contents\\n  URL = \"https://auctree.com/en/auction/\"+number #URL\\n  r = requests.get(URL) \\n  soup = BeautifulSoup(r.content, \\'html5lib\\')\\n  image = soup.find(\\'img\\', attrs = {\\'class\\': \\'MuiCardMedia-root MuiCardMedia-media MuiCardMedia-img css-o69gx8-MuiCardMedia-root\\'})[\\'src\\']\\n  print(image)\\n  images.append(image)\\n\\n\\nimport pandas as pd\\nprop_df = pd.DataFrame([numbers, names, images,  prop_types, regions, cities, sale_types, auc_prices, time_lefts, end_dates]).T\\nprop_df.columns = [\\'number\\', \\'name\\', \\'image\\', \\'prop_type\\', \\'region\\', \\'city\\', \\'sale_type\\', \\'auc_price\\', \\'time_left\\', \\'end_date\\']\\nprop_dict = prop_df.T.to_dict()\\nprint(prop_dict)\\n\\n\\nprint(prop_df[:])\\n\\ncsv_name = \\'/content/auctree_data.csv\\'\\nprop_df.to_csv(csv_name, index= False)\\njson_name = \\'/content/auctree_data.json\\'\\nprop_df.T.to_json(json_name)\\nprint(\\'completed\\')\\n\\n\\n'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "#Python program to scrape website \n",
        "#and save data from website \n",
        "import requests \n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
        "HTML(styles)\n",
        "\n",
        "#GET HTML DOM contents\n",
        "URL = \"https://auctree.com/en/\"#URL for getting BDC rates from abokiFX.com\n",
        "r = requests.get(URL) #response to the HTTP request on the abokiFX site\n",
        "soup = BeautifulSoup(r.content, 'html5lib') #'soupify' for traversal using bs4\n",
        "\n",
        "\n",
        "body = soup.find('div', attrs = {'class': 'css-t23ds5'}) \n",
        "\n",
        "print(body)\n",
        "\n",
        "names = []\n",
        "names_ = body.findAll('span', attrs = {'id': 'SearchTableName'}) \n",
        "for name in names_:\n",
        "  names.append(name.text)\n",
        "\n",
        "prop_types = []\n",
        "prop_types_ = body.findAll('span', attrs = {'id': 'SearchTablePropertyTypes'}) \n",
        "for prop_type in prop_types_:\n",
        "  prop_types.append(prop_type.text)\n",
        "\n",
        "regions = []\n",
        "regions_ = body.findAll('span', attrs = {'id': 'SearchTableRegion'})\n",
        "for region in regions_:\n",
        "  regions.append(region.text)\n",
        "\n",
        "cities = []\n",
        "cities_ = body.findAll('span', attrs = {'id': 'SearchTableCities'})\n",
        "for city in cities_:\n",
        "  cities.append(city.text)\n",
        "\n",
        "sale_types = []\n",
        "sale_types_ = body.findAll('span', attrs = {'id': 'SearchTableSaleType'}) \n",
        "for sale_type in sale_types_:\n",
        "  sale_types.append(sale_type.text)\n",
        "\n",
        "auc_prices = []\n",
        "auc_prices_ = body.findAll('span', attrs = {'id': 'SearchTableAuctionPrice'}) \n",
        "for auc_price in auc_prices_:\n",
        "  auc_prices.append(auc_price.text)\n",
        "\n",
        "time_lefts = []\n",
        "time_lefts_ = body.findAll('span', attrs = {'id': 'SearchTableUntil'}) \n",
        "for time_left in time_lefts_:\n",
        "  time_lefts.append(time_left.text)\n",
        "\n",
        "end_dates = []\n",
        "end_dates_ = body.findAll('span', attrs = {'id': 'SearchTableEndDate'}) \n",
        "for end_date in end_dates_:\n",
        "  end_dates.append(end_date.text)\n",
        "\n",
        "numbers = []\n",
        "for name in names:\n",
        "  number = name[:name.find('.')]\n",
        "  numbers.append(number)\n",
        "\n",
        "numbers = []\n",
        "for i in range(len(names)):\n",
        "  number = (names[i])[:names[i].find('.')]\n",
        "  names[i] = (names[i])[names[i].find('Cl')+3:]# ignore number + cl prefix\n",
        "  numbers.append(number)\n",
        "\n",
        "images = []\n",
        "for number in numbers:\n",
        "  #GET HTML DOM contents\n",
        "  URL = \"https://auctree.com/en/auction/\"+number #URL\n",
        "  r = requests.get(URL) \n",
        "  soup = BeautifulSoup(r.content, 'html5lib')\n",
        "  image = soup.find('img', attrs = {'class': 'MuiCardMedia-root MuiCardMedia-media MuiCardMedia-img css-o69gx8-MuiCardMedia-root'})['src']\n",
        "  print(image)\n",
        "  images.append(image)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "prop_df = pd.DataFrame([numbers, names, images,  prop_types, regions, cities, sale_types, auc_prices, time_lefts, end_dates]).T\n",
        "prop_df.columns = ['number', 'name', 'image', 'prop_type', 'region', 'city', 'sale_type', 'auc_price', 'time_left', 'end_date']\n",
        "prop_dict = prop_df.T.to_dict()\n",
        "print(prop_dict)\n",
        "\n",
        "\n",
        "print(prop_df[:])\n",
        "\n",
        "csv_name = '/content/auctree_data.csv'\n",
        "prop_df.to_csv(csv_name, index= False)\n",
        "json_name = '/content/auctree_data.json'\n",
        "prop_df.T.to_json(json_name)\n",
        "print('completed')\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVH5Z1xQ9yRI"
      },
      "source": [
        "#**Attempt 2: Scraping individual property pages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTCLnHAIueua"
      },
      "outputs": [],
      "source": [
        "#Python program to scrape website \n",
        "#and save data from website\n",
        "import requests \n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.core.display import HTML\n",
        "import mechanize\n",
        "import http\n",
        "\n",
        "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
        "HTML(styles)\n",
        "\n",
        "\n",
        "properties = []\n",
        "#loop throught the pages and get relevant field values\n",
        "for i in range(10,1500):\n",
        "  #open the web page \n",
        "  URL = \"https://auctree.com/en/auction/\"+str(i) #URL\n",
        "  r = requests.get(URL)\n",
        "  soup = BeautifulSoup(r.content, 'html5lib')\n",
        "  data = {}\n",
        "\n",
        "  #retireve the field values\n",
        "  try:\n",
        "    #retrieve property number and name(address)\n",
        "    name = soup.find('h4', attrs = {'class': 'MuiTypography-root MuiTypography-h4 css-xbbocf-MuiTypography-root'}).getText()\n",
        "    data['number'] = name[:name.find(\".\")].strip()\n",
        "    data['name'] = name[name.find(\". \")+1:].strip()\n",
        "  except Exception as e:\n",
        "    data['number'] = \"\"\n",
        "    data['name'] = \"\"\n",
        "  try:\n",
        "    #retrieve and clean region, city, and prop_type....*****NOTE: Region and city have been accidentally swapped ******FIX!\n",
        "    location = soup.find('p', attrs = {'class': 'MuiTypography-root MuiTypography-body2 css-19mp1k9-MuiTypography-root'}).getText()\n",
        "    data['region'] = location[:location.find(',')].strip()\n",
        "    data['city'] = location[location.find(',')+1:location.find('·')].strip()\n",
        "    data['prop_type'] = location[location.find('·')+1:].strip()\n",
        "  except Exception as e:\n",
        "    data['region'] = \"\"\n",
        "    data['city'] = \"\"\n",
        "    data['prop_type'] = \"\"\n",
        "  try:\n",
        "    #retrive time until auction end\n",
        "    data['time_left'] = soup.find('p', attrs = {'class': 'MuiTypography-root MuiTypography-body2 css-1hv23l3-MuiTypography-root'}).getText()\n",
        "  except Exception as e:\n",
        "    data['time_left'] = \"\"\n",
        "  try:\n",
        "    #retrieve action close date\n",
        "    data['close_date'] = soup.find('p', attrs = {'class': 'MuiTypography-root MuiTypography-body2 css-1hv23l3-MuiTypography-root'}).getText()\n",
        "  except Exception as e:\n",
        "    data['close_date'] = \"\"\n",
        "  try:\n",
        "    #retrive and clean start bid\n",
        "    start_bid = soup.find('h6', attrs = {'class': 'MuiTypography-root MuiTypography-h6 css-t7sa6x-MuiTypography-root'}).getText()\n",
        "    if start_bid[0] == '€':\n",
        "      data['start_bid'] = float(start_bid[start_bid.find('€')+1:])\n",
        "    elif start_bid[-1] == '€':\n",
        "      data['start_bid'] = float(start_bid[:start_bid.find('\\\\xa0')-1])\n",
        "  except Exception as e:\n",
        "    data['start_bid'] = \"\"\n",
        "  try:\n",
        "    #retirve and clean deposit\n",
        "    bod = soup.find('div', attrs = {'class': 'css-1mq1uye'})\n",
        "    deposit = bod.find('p', attrs = {'class': 'MuiTypography-root MuiTypography-body2 css-19mp1k9-MuiTypography-root'}).getText()\n",
        "    data['deposit'] = float((deposit[:deposit.find('%')]).replace(',','.'))/100\n",
        "  except Exception as e:\n",
        "    data['deposit'] = \"\"\n",
        "  try:\n",
        "    #retrieve asale type\n",
        "    bod = soup.find('div', attrs = {'class': 'css-1ehc112'})\n",
        "    data['sale_type'] = bod.find('span', attrs = {'class': 'MuiTypography-root MuiTypography-body3 MuiTypography-noWrap css-1r5mk5o-MuiTypography-root'}).getText()\n",
        "  except Exception as e:\n",
        "    data['sale_type'] = \"\"\n",
        "  try:\n",
        "    #retieve image source address\n",
        "    data['image'] = soup.find('img', attrs = {'class': 'MuiCardMedia-root MuiCardMedia-media MuiCardMedia-img css-o69gx8-MuiCardMedia-root'})['src']\n",
        "  except Exception as e:\n",
        "    data['image'] = \"\"\n",
        "  try:\n",
        "    #retirve and clean buy it now price\n",
        "    buy_now = soup.find('h6', attrs = {'class': 'MuiTypography-root MuiTypography-h6 css-859x9-MuiTypography-root'}).getText()\n",
        "    if buy_now[0] == '€':\n",
        "      data['buy_now'] = float(buy_now[buy_now.find('€')+1:])\n",
        "    elif buy_now[-1] == '€':\n",
        "      data['buy_now'] = float(buy_now[:buy_now.find('\\\\xa0')-1])\n",
        "  except Exception as e:\n",
        "    data['buy_now'] = \"\"\n",
        "  properties.append(data)\n",
        "  print(i)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwoze2QJ99Qm"
      },
      "source": [
        "#**Store in flatefile and upload to cloud storage for analytics in Synapse**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hg0qyM61kih"
      },
      "outputs": [],
      "source": [
        "from pandas.io.json import json_normalize\n",
        "flat_data = json_normalize(properties)\n",
        "#load json to csv\n",
        "csv_filename = 'auctree_big.csv'\n",
        "flat_data.to_csv(csv_filename)\n",
        "\n",
        "#upload the data to Azure Storage\n",
        "import os\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n",
        "\n",
        "try:\n",
        "    print(\"Starting upload process...\")\n",
        "    # Retrieve the connection string for use with the application. The storage\n",
        "    connect_str = AZR_CONN_STR\n",
        "    # Create the BlobServiceClient object which will be used to create a container client\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "    # Create a unique name for the container\n",
        "    container_name = \"mauctree\" #+ str(uuid.uuid4())\n",
        "    try:\n",
        "      container_client = blob_service_client.create_container(container_name)\n",
        "    except Exception as e:\n",
        "      print('Storage container creation exception:')\n",
        "      print(e)\n",
        "    \n",
        "    #upload the files\n",
        "    \n",
        "    try:\n",
        "      # Create a file in the local data directory to upload and download\n",
        "      local_path = '/content'\n",
        "      local_file_name = csv_filename\n",
        "      upload_file_path = os.path.join(local_path, local_file_name)\n",
        "\n",
        "      # Create a blob client using the local file name as the name for the blob\n",
        "      blob_client = blob_service_client.get_blob_client(container=container_name, blob=upload_file_path)\n",
        "\n",
        "      print(\"\\nUploading to Azure Storage as blob:\\n\\t\" + local_file_name)\n",
        "\n",
        "      # Upload the created file\n",
        "      with open(upload_file_path, \"rb\") as data:\n",
        "          blob_client.upload_blob(data)\n",
        "          \n",
        "    except Exception as e:\n",
        "      print('Upload to container exception:')\n",
        "      print(e)\n",
        "\n",
        "\n",
        "    # List the blobs in the container\n",
        "    print(\"\\nListing blobs...\")\n",
        "    blob_list = container_client.list_blobs()\n",
        "    for blob in blob_list:\n",
        "        print(\"\\t\" + blob.name)\n",
        "        \n",
        "except Exception as e:\n",
        "    print('Exception: ')\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Upload to to database**"
      ],
      "metadata": {
        "id": "DKH8weP7kM2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV4IortmZnbS"
      },
      "outputs": [],
      "source": [
        "# Use a service \n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "import datetime \n",
        "import time\n",
        "cred = credentials.Certificate('/content/shopmob-secret.json')\n",
        "try:\n",
        "  firebase_admin.initialize_app(cred)\n",
        "except:\n",
        "  print('error occurred in initialising firebase. Perhaps already initialised')\n",
        "\n",
        "db = firestore.client()\n",
        "now = datetime.datetime.now()\n",
        "nowCalc = time.mktime(now.timetuple())\n",
        "#post to db\n",
        "for propertie in properties:\n",
        "  response = db.collection(u'mauctree_properties').document().set(propertie)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9QX5PH2HZMr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-IaQi0wZD84"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyOR7//ZTNbW03MVctXhj0mI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}